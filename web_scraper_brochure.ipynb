{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Web Scraper to Collect Data and Generate Brochure\n", "This notebook demonstrates how to create a web scraper tool that collects website data and generates a brochure. It uses Python libraries like `requests`, `BeautifulSoup`, and `FPDF`."]}, {"cell_type": "code", "metadata": {}, "source": ["# Import required libraries\n", "import requests\n", "from bs4 import BeautifulSoup\n", "from fpdf import FPDF\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 1: Scrape Website Data\n", "We will begin by scraping the data from a website, extracting its title and paragraphs."]}, {"cell_type": "code", "metadata": {}, "source": ["# Function to scrape the website\n", "def scrape_website(url):\n", "    response = requests.get(url)\n", "    soup = BeautifulSoup(response.text, 'html.parser')\n", "    title = soup.title.string if soup.title else 'No Title Found'\n", "    paragraphs = [p.get_text() for p in soup.find_all('p')]\n", "    return title, paragraphs\n", "\n", "# Example URL\n", "url = 'https://example.com'  # Replace with the website URL\n", "title, paragraphs = scrape_website(url)\n", "\n", "# Print scraped data\n", "print(f'Title: {title}')\n", "for para in paragraphs:\n", "    print(para)"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 2: Process Data for Brochure\n", "Now we will process the data by cleaning the paragraphs and preparing them for the brochure."]}, {"cell_type": "code", "metadata": {}, "source": ["# Function to process the data\n", "def process_data(title, paragraphs):\n", "    processed_paragraphs = [para.strip() for para in paragraphs if len(para.strip()) > 50]\n", "    return {\n", "        'title': title,\n", "        'paragraphs': processed_paragraphs\n", "    }\n", "\n", "# Example processing\n", "processed_data = process_data(title, paragraphs)\n", "print(processed_data)"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 3: Create a PDF Brochure\n", "We will use `fpdf` to format the processed data into a PDF brochure."]}, {"cell_type": "code", "metadata": {}, "source": ["# Function to create the brochure\n", "def create_brochure(data, filename='brochure.pdf'):\n", "    pdf = FPDF()\n", "    pdf.set_auto_page_break(auto=True, margin=15)\n", "    pdf.add_page()\n", "    pdf.set_font('Arial', 'B', 16)\n", "    pdf.cell(200, 10, txt=data['title'], ln=True, align='C')\n", "    pdf.ln(10)\n", "    pdf.set_font('Arial', size=12)\n", "    for para in data['paragraphs']:\n", "        pdf.multi_cell(0, 10, txt=para)\n", "    pdf.output(filename)\n", "\n", "# Create the brochure\n", "create_brochure(processed_data)"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 4: Integrate Database for Storing Scraped Data\n", "We will now create a simple database using SQLite to store the scraped data."]}, {"cell_type": "code", "metadata": {}, "source": ["# Function to initialize the database\n", "import sqlite3\n", "\n", "def initialize_db(db_name='scraped_data.db'):\n", "    conn = sqlite3.connect(db_name)\n", "    cursor = conn.cursor()\n", "    cursor.execute('''CREATE TABLE IF NOT EXISTS webpages (\n", "                        id INTEGER PRIMARY KEY,\n", "                        title TEXT,\n", "                        content TEXT\n", "                    )''')\n", "    conn.commit()\n", "    conn.close()\n", "\n", "# Function to store scraped data\n", "def store_data(title, paragraphs, db_name='scraped_data.db'):\n", "    conn = sqlite3.connect(db_name)\n", "    cursor = conn.cursor()\n", "    content = ' '.join(paragraphs)\n", "    cursor.execute('INSERT INTO webpages (title, content) VALUES (?, ?)', (title, content))\n", "    conn.commit()\n", "    conn.close()\n", "\n", "# Initialize the database\n", "initialize_db()\n", "\n", "# Store scraped data into the database\n", "store_data(title, paragraphs)\n", "print('Data stored in the database successfully.')"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Final Tool\n", "We can now combine everything into a tool that scrapes data, processes it, generates a brochure, and stores the data in a database."]}, {"cell_type": "code", "metadata": {}, "source": ["# Final function to generate brochure and store data\n", "def generate_brochure_from_website(url, filename='brochure.pdf', db_name='scraped_data.db'):\n", "    title, paragraphs = scrape_website(url)\n", "    processed_data = process_data(title, paragraphs)\n", "    create_brochure(processed_data, filename)\n", "    store_data(title, paragraphs, db_name)\n", "    print(f'Brochure created and data stored for: {url}')\n", "\n", "# Example usage\n", "generate_brochure_from_website('https://example.com')"], "outputs": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "version": "3.8.8"}}, "nbformat": 4, "nbformat_minor": 5}