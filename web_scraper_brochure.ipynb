{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "838cfc98",
   "metadata": {},
   "source": [
    "# Web Scraper Tool for Data Collection and Brochure Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaec56c3",
   "metadata": {},
   "source": [
    "\n",
    "## Overview\n",
    "This repository contains a Python-based web scraper tool that extracts data from websites and generates a brochure. The tool scrapes essential content like titles and paragraphs from a specified website and creates a well-formatted PDF brochure. Additionally, the data is stored in an SQLite database for further use.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Web Scraping**: Scrapes website content, including titles and paragraphs.\n",
    "- **Brochure Generation**: Creates a PDF brochure using the scraped content.\n",
    "- **Database Integration**: Stores the scraped website data in an SQLite database for future reference.\n",
    "- **Customizable**: Can be adapted to scrape any website by specifying the URL.\n",
    "\n",
    "## Technologies Used\n",
    "\n",
    "- **Python**: The main programming language used for the project.\n",
    "- **BeautifulSoup**: For parsing and scraping HTML data from websites.\n",
    "- **Requests**: For making HTTP requests to fetch web pages.\n",
    "- **FPDF**: For creating the PDF brochure from the scraped data.\n",
    "- **SQLite**: For storing scraped data in a database.\n",
    "\n",
    "## Installation\n",
    "\n",
    "### 1. Clone the repository\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/yourusername/web-scraper-brochure.git\n",
    "cd web-scraper-brochure\n",
    "```\n",
    "\n",
    "### 2. Install dependencies\n",
    "\n",
    "Make sure you have Python 3 installed. Then, install the required libraries:\n",
    "\n",
    "```bash\n",
    "pip install requests beautifulsoup4 fpdf\n",
    "```\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Scrape Data and Generate Brochure\n",
    "\n",
    "To use the scraper and generate a brochure, simply run the following command in your Python environment:\n",
    "\n",
    "```python\n",
    "from scraper import generate_brochure_from_website\n",
    "\n",
    "# Provide the URL of the website you want to scrape\n",
    "generate_brochure_from_website('https://example.com', filename='brochure.pdf')\n",
    "```\n",
    "\n",
    "This will:\n",
    "1. Scrape the website content.\n",
    "2. Generate a PDF brochure named `brochure.pdf`.\n",
    "3. Store the scraped data in an SQLite database for future use.\n",
    "\n",
    "## Database\n",
    "\n",
    "The scraped data is stored in an SQLite database named `scraped_data.db`. The database contains the following table:\n",
    "\n",
    "- **webpages**:\n",
    "  - `id`: Primary key\n",
    "  - `title`: Title of the webpage\n",
    "  - `content`: Content scraped from the webpage (i.e., paragraphs)\n",
    "\n",
    "## Contributing\n",
    "\n",
    "1. Fork the repository.\n",
    "2. Create your feature branch (`git checkout -b feature-name`).\n",
    "3. Commit your changes (`git commit -am 'Add feature'`).\n",
    "4. Push to the branch (`git push origin feature-name`).\n",
    "5. Create a new Pull Request.\n",
    "\n",
    "## License\n",
    "\n",
    "This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "- [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/)\n",
    "- [FPDF](http://www.fpdf.org/)\n",
    "- [SQLite](https://www.sqlite.org/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f58824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fpdf import FPDF\n",
    "import sqlite3\n",
    "\n",
    "def scrape_website(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    title = soup.title.string if soup.title else 'No Title Found'\n",
    "    paragraphs = [p.get_text() for p in soup.find_all('p')]\n",
    "    return title, paragraphs\n",
    "\n",
    "def process_data(title, paragraphs):\n",
    "    processed_paragraphs = [para.strip() for para in paragraphs if len(para.strip()) > 50]\n",
    "    return {'title': title, 'paragraphs': processed_paragraphs}\n",
    "\n",
    "def create_brochure(data, filename='brochure.pdf'):\n",
    "    pdf = FPDF()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    pdf.add_page()\n",
    "    pdf.set_font('Arial', 'B', 16)\n",
    "    pdf.cell(200, 10, txt=data['title'], ln=True, align='C')\n",
    "    pdf.ln(10)\n",
    "    pdf.set_font('Arial', size=12)\n",
    "    for para in data['paragraphs']:\n",
    "        pdf.multi_cell(0, 10, txt=para)\n",
    "    pdf.output(filename)\n",
    "\n",
    "def initialize_db(db_name='scraped_data.db'):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''CREATE TABLE IF NOT EXISTS webpages (\n",
    "                        id INTEGER PRIMARY KEY,\n",
    "                        title TEXT,\n",
    "                        content TEXT\n",
    "                    )''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def store_data(title, paragraphs, db_name='scraped_data.db'):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    content = ' '.join(paragraphs)\n",
    "    cursor.execute('INSERT INTO webpages (title, content) VALUES (?, ?)', (title, content))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def generate_brochure_from_website(url, filename='brochure.pdf', db_name='scraped_data.db'):\n",
    "    title, paragraphs = scrape_website(url)\n",
    "    processed_data = process_data(title, paragraphs)\n",
    "    create_brochure(processed_data, filename)\n",
    "    store_data(title, paragraphs, db_name)\n",
    "    print(f'Brochure created and data stored for: {url}')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
